{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Mining: Basic Concepts - WS'22/23\n",
    "---------------\n",
    "``` \n",
    "> University of Konstanz \n",
    "> Department of Computer and Information Science\n",
    "> Prof. Dr. Daniel Keim, Eren Cakmak, Raphael Buchm√ºller, Udo Schlegel, Yannick Metz\n",
    "```\n",
    "__Organize in teams of 2 people, return the exercise by Sun, Jan 15th, 2022 (11:59 PM) using ILIAS__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 08 in Python \n",
    "---------------\n",
    "- ___Please put your names and student IDs here___:\n",
    "    - _Name_, _Student ID_ Arsen Ukrainets, Matriculation No.1296653\n",
    "    - _Name_, _Student ID_ Serhii Kovalov "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 1: Lazy vs Eager: Theory (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Explain the difference between an eager learner and a lazy learner (1P).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An eager learner is a machine learning algorithm that immediately starts the learning process using the available data, while a lazy learner waits until it receives new data before starting the learning process. Eager learners are generally more computationally expensive and may require more memory resources than lazy learners. However, they are able to learn from the data more quickly and can provide better performance. Lazy learners, on the other hand, are more computationally efficient and typically require less memory resources, but may take longer to learn from the data and may not provide as good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Provide an example of a machine learning algorithm that is an eager learner and one that is a lazy learner (1P).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a machine learning algorithm that is an lazy learner is the k-Nearest Neighbors (k-NN) algorithm.\n",
    "An example of a machine learning algorithm that is an eager learner is A decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a dataset with 1 million rows and 10 columns. You need to build a machine learning model to predict a binary outcome based on the values in the columns. You have two options: a decision tree or a k-nearest neighbors algorithm.   \n",
    "  \n",
    "**b) Which algorithm should you choose and why? Explain your reasoning in detail, taking into account the size and complexity of the dataset and the computational resources available to you. (2P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would using a decision tree algorithm. \n",
    "Because: \n",
    "1. it is able to handle large datasets\n",
    "2. it's easy to interpret\n",
    "3. it can handle non-linear relationships. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 2: Lazy vs Eager: Practical (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we want to implement a basic k-nearest neighbor algorithm.   \n",
    "For this, we need a similarity function and a general k-nearest neighbor function.  \n",
    "We will use the breast cancer dataset we previously used for the SVM with an 80/20 train test split.    \n",
    "  \n",
    "**Please implement the k-nearest neighbor algorithm on your own and show that the implementation works for the breast cancer data. (6P)**  \n",
    "_(Hint: use the `sklearn.model_selection.train_test_split` method and the parameter `random_state=0`)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9298245614035088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arsen\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the breast cancer data set\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a k-NN classifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifier\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the classifier\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 3: Evaluation Metrics Theory (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Explain why different evaluation metrics are used for evaluating the performance of classifiers (1P).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different evaluation metrics are used for evaluating the performance of classifiers because different metrics focus on different aspects of the model's performance, and what is considered important may depend on the specific use case or context.\n",
    "Accuracy, Precision, Sensitivity etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Provide examples of common evaluation metrics for classification algorithms, and explain the strengths and limitations of each metric in the context of classifier evaluation. (2P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: This metric measures the proportion of correct predictions made by the classifier. It is easy to understand and interpret, but it can be misleading in cases where the class distribution is imbalanced. For example, if the data is skewed with 90% of the samples belonging to one class, a model that predicts all samples belong to that class will have an accuracy of 90%, but it won't be a good model.\n",
    "\n",
    "Precision: This metric measures the proportion of true positive predictions among all positive predictions made by the classifier. It is useful when the cost of false positives is high, such as in medical diagnosis. Precision gives you an idea of how many of the positive predictions your classifier made are actually correct. However, precision alone may not be sufficient in cases where the goal is to identify as many positive instances as possible, such as in fraud detection.\n",
    "\n",
    "Sensitivity (Recall): This metric measures the proportion of true positive predictions among all actual positive instances. It is useful when the cost of false negatives is high, such as in fraud detection. Recall gives you an idea of how many of the actual positive instances your classifier was able to identify. However, recall alone may not be sufficient in cases where the goal is to minimize the number of false positives, such as in medical diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Explain how a perfect precision-recall curve looks like. (1P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect precision-recall curve would be a graph that shows a recall of 1.0 (or 100%) on the y-axis and a precision of 1.0 (or 100%) on the x-axis. In other words, a perfect precision-recall curve would have a point located at the top-right corner of the graph, with a coordinate of (1.0, 1.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 4: Evaluation Metrics Practical (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After thinking about the different evaluation metrics, we want to see them in real-world scenarios and inspect their workings.  \n",
    "We tackle two datasets for this task and use the machine learning algorithms we have already explored.  \n",
    "The first dataset will be the breast cancer dataset again. The second one will be the cover-type forest dataset.  \n",
    "Both datasets have unique properties and need special care. Handling different data is not always easy and selecting proper algorithms is even more challenging.\n",
    "Thus, we focus on the different evaluation metrics to select one good working algorithm.  \n",
    "In this case, we will compare the sci-kit learn implementations of Naive Bayes, Decision Trees, SVMs, and k-Nearest Neighbors based on the classification accuracy and F1-score.  \n",
    "  \n",
    "As the cover type data has more than two classes, we will only use the second and third class samples.  \n",
    "  \n",
    "**Please implement a comparison between the sci-kit learn implementations of Naive Bayes, Decision Trees, SVMs, and k-Nearest Neighbors on the breast cancer data and cover type data for your own implementation of accuracy and F1-score. (6P)**    \n",
    "_(Hint: use the `sklearn.model_selection.train_test_split` method and the parameter `random_state=0`)_  \n",
    "_(Hint: use the `sklearn.naive_bayes.GaussianNB`, `sklearn.tree.DecisionTreeClassifier`, `sklearn.svm.SVC`, `sklearn.neighbors.KNeighborsClassifier` methods with default parameters)_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9649122807017544\n",
      "0.9736842105263158\n",
      "0.9385964912280702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arsen\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the breast cancer data set\n",
    "data = load_breast_cancer()\n",
    "X_breast_cancer = data.data\n",
    "y_breast_cancer = data.target\n",
    "\n",
    "# Load the covertype data set\n",
    "data = fetch_covtype()\n",
    "X_covertype = data.data\n",
    "y_covertype = data.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_breast_cancer, X_test_breast_cancer, y_train_breast_cancer, y_test_breast_cancer = train_test_split(X_breast_cancer, y_breast_cancer, test_size=0.2, random_state=42)\n",
    "X_train_covertype, X_test_covertype, y_train_covertype, y_test_covertype = train_test_split(X_covertype, y_covertype, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Create a Support Vector Machine classifier\n",
    "svm = svm.SVC()\n",
    "\n",
    "# Create a k-NN classifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifiers\n",
    "gnb.fit(X_train_breast_cancer, y_train_breast_cancer)\n",
    "dt.fit(X_train_breast_cancer, y_train_breast_cancer)\n",
    "svm.fit(X_train_breast_cancer, y_train_breast_cancer)\n",
    "knn.fit(X_train_breast_cancer, y_train_breast_cancer)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gnb_breast_cancer = gnb.predict(X_test_breast_cancer)\n",
    "y_pred_dt_breast_cancer = dt.predict(X_test_breast_cancer)\n",
    "y_pred_svm_breast_cancer = svm.predict(X_test_breast_cancer)\n",
    "y_pred_knn_breast_cancer = knn.predict(X_test_breast_cancer)\n",
    "\n",
    "# Print the accuracy of the classifiers\n",
    "print(accuracy_score(y_pred_gnb_breast_cancer, y_pred_gnb_breast_cancer))\n",
    "print(accuracy_score(y_pred_gnb_breast_cancer, y_pred_dt_breast_cancer))\n",
    "print(accuracy_score(y_pred_gnb_breast_cancer, y_pred_svm_breast_cancer))\n",
    "print(accuracy_score(y_pred_gnb_breast_cancer, y_pred_knn_breast_cancer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "116075abdc1ed53ddb80f9e3462a1a87c17f9d858d16aae38864710ebb8faa39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
